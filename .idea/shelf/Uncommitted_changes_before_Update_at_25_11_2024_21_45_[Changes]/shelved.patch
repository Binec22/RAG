Index: python_script/config.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n    \"default\": {\r\n        \"data_path\": \"data/documents/default\",\r\n        \"chroma_root_path\": \"data/chroma/default\",\r\n        \"embedding_model\": \"voyage-law-2\",\r\n        \"llm_model\": \"gpt-3.5-turbo\",\r\n        \"prompt_template\": \"Answer the question based only on the following context:\\n\\n{context}\\n\\n---\\n\\nAnswer the question based on the above context: {question}\"\r\n    },\r\n    \"seus\": {\r\n      \"data_path\": \"data/documents/ship_data\",\r\n      \"chroma_root_path\": \"data/chroma/ship_chroma\",\r\n      \"embedding_model\": \"voyage-multilingual-2\",\r\n      \"llm_model\": \"gpt-3.5-turbo\",\r\n      \"prompt_template\": \"Answer the question based only on the following context:\\n\\n{context}\\n\\n---\\n\\nAnswer the question based on the above context: {question}\"\r\n    },\r\n    \"arch_en\": {\r\n      \"data_path\": \"data/documents/arch_data-en\",\r\n      \"chroma_root_path\": \"data/chroma/arch_data-en_chroma\",\r\n      \"embedding_model\": \"openai\",\r\n      \"llm_model\": \"gpt-3.5-turbo\",\r\n      \"prompt_template\": \"Answer the question based only on the following context:\\n\\n{context}\\n\\n---\\n\\nPlease provide your answer to the question: {question}\"\r\n    },\r\n    \"arch_ru\": {\r\n      \"data_path\": \"data/documents/arch_data-ru\",\r\n      \"chroma_root_path\": \"data/chroma/arch_data-ru_chroma\",\r\n      \"embedding_model\": \"voyage-multilingual-2\",\r\n      \"llm_model\": \"gpt-3.5-turbo\",\r\n      \"prompt_template\": \"Answer the question based only on the following context:\\n\\n{context}\\n\\n---\\n\\nPlease provide your answer to the question: {question}\"\r\n    },\r\n    \"test\": {\r\n      \"data_path\": \"data/test/documents\",\r\n      \"chroma_root_path\": \"data/test/embedded_database\",\r\n      \"embedding_model\": \"voyage-3\",\r\n      \"llm_model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\r\n      \"prompt_template\": \"Answer the question based only on the following context:\\n\\n{context}\\n\\n---\\n\\nPlease provide your answer to the question: {question}\"\r\n    }\r\n  }\r\n  
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python_script/config.json b/python_script/config.json
--- a/python_script/config.json	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
+++ b/python_script/config.json	(date 1731621895637)
@@ -31,7 +31,7 @@
       "data_path": "data/test/documents",
       "chroma_root_path": "data/test/embedded_database",
       "embedding_model": "voyage-3",
-      "llm_model": "mistralai/Mistral-7B-Instruct-v0.1",
+      "llm_model": "BEE-spoke-data/smol_llama-101M-GQA",
       "prompt_template": "Answer the question based only on the following context:\n\n{context}\n\n---\n\nPlease provide your answer to the question: {question}"
     }
   }
Index: python_script/get_rag_chain.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from parameters import CHROMA_ROOT_PATH, EMBEDDING_MODEL, LLM_MODEL\r\n\r\nfrom get_embedding_function import get_embedding_function\r\nfrom get_llm_function import get_llm_function\r\nfrom populate_database import find_chroma_path\r\n\r\nfrom langchain_chroma import Chroma\r\n\r\nfrom langchain.chains import create_history_aware_retriever\r\nfrom langchain.chains import create_retrieval_chain\r\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\r\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\n\r\n\r\ndef get_rag_chain(params=None):\r\n    default_params = {\r\n        # \"chroma_root_path\": CHROMA_ROOT_PATH,\r\n        # \"embedding_model\": EMBEDDING_MODEL,\r\n        # \"llm_model\": LLM_MODEL,\r\n        \"chroma_root_path\": \"data/test/embedded_database\",\r\n        \"embedding_model\": \"voyage-3\",\r\n        \"llm_model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\r\n        \"search_type\": \"similarity\",\r\n        \"similarity_doc_nb\": 5,\r\n        \"score_threshold\": 0.8,\r\n        \"max_chunk_return\": 5,\r\n        \"considered_chunk\": 25,\r\n        \"mmr_doc_nb\": 5,\r\n        \"lambda_mult\": 0.25,\r\n        \"isHistoryOn\": True,\r\n    }\r\n    if params is None:\r\n        params = default_params\r\n    else:\r\n        params = {**default_params, **params}\r\n\r\n    try:\r\n        required_keys = [\"chroma_root_path\", \"embedding_model\", \"llm_model\"]\r\n        for key in required_keys:\r\n            if key not in params:\r\n                raise NameError(f\"Required setting '{key}' not defined.\")\r\n\r\n        embedding_model = get_embedding_function(model_name=params[\"embedding_model\"])\r\n        llm = get_llm_function(model_name=params[\"llm_model\"])\r\n        db = Chroma(persist_directory=find_chroma_path(model_name=params[\"embedding_model\"],\r\n                                                       base_path=params[\"chroma_root_path\"]),\r\n                    embedding_function=embedding_model)\r\n\r\n        search_type = params[\"search_type\"]\r\n        if search_type == \"similarity\":\r\n            retriever = db.as_retriever(search_type=search_type,\r\n                                        search_kwargs={\"k\": params[\"similarity_doc_nb\"]})\r\n\r\n        elif search_type == \"similarity_score_threshold\":\r\n            retriever = db.as_retriever(search_type=search_type,\r\n                                        search_kwargs={\"k\": params[\"max_chunk_return\"],\r\n                                                       \"score_threshold\": params[\"score_threshold\"]})\r\n\r\n        elif search_type == \"mmr\":\r\n            retriever = db.as_retriever(search_type=search_type,\r\n                                        search_kwargs={\"k\": params[\"mmr_doc_nb\"], \"fetch_k\": params[\"considered_chunk\"],\r\n                                                       \"lambda_mult\": params[\"lambda_mult\"]})\r\n\r\n        else:\r\n            raise ValueError(\"Invalid 'search_type' setting\")\r\n\r\n    except NameError as e:\r\n        variable_name = str(e).split(\"'\")[1]\r\n        raise NameError(f\"{variable_name} isn't defined\")\r\n\r\n    if params[\"isHistoryOn\"]:\r\n        contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\r\n        which might reference context in the chat history, formulate a standalone question \\\r\n        which can be understood without the chat history. Do NOT answer the question, \\\r\n        just reformulate it if needed and otherwise return it as is.\"\"\"\r\n\r\n        contextualize_q_prompt = ChatPromptTemplate.from_messages(\r\n            [\r\n                (\"system\", contextualize_q_system_prompt),\r\n                MessagesPlaceholder(\"chat_history\"),\r\n                (\"human\", \"{input}\"),\r\n            ]\r\n        )\r\n\r\n        history_aware_retriever = create_history_aware_retriever(\r\n            llm, retriever, contextualize_q_prompt\r\n        )\r\n        retriever = history_aware_retriever\r\n\r\n    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\r\n    Use the following pieces of retrieved context to answer the question. \\\r\n    If you don't know the answer, just say that you don't know. \\\r\n    Use three sentences maximum and keep the answer concise.\\\r\n\r\n    {context}\"\"\"\r\n    qa_prompt = ChatPromptTemplate.from_messages(\r\n        [\r\n            (\"system\", qa_system_prompt),\r\n            MessagesPlaceholder(\"chat_history\"),\r\n            (\"human\", \"{input}\"),\r\n        ]\r\n    )\r\n\r\n    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\r\n\r\n    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\r\n\r\n    return rag_chain\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python_script/get_rag_chain.py b/python_script/get_rag_chain.py
--- a/python_script/get_rag_chain.py	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
+++ b/python_script/get_rag_chain.py	(date 1731622227443)
@@ -19,7 +19,7 @@
         # "llm_model": LLM_MODEL,
         "chroma_root_path": "data/test/embedded_database",
         "embedding_model": "voyage-3",
-        "llm_model": "mistralai/Mistral-7B-Instruct-v0.1",
+        "llm_model": "crumb/nano-mistral",
         "search_type": "similarity",
         "similarity_doc_nb": 5,
         "score_threshold": 0.8,
Index: python_script/get_llm_function.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    pipeline\r\n)\r\n\r\nfrom langchain_huggingface import HuggingFacePipeline\r\n\r\nfrom parameters import LLM_MODEL\r\n\r\nglobal_model = None\r\n\r\nimport os\r\n\r\nhf_token = os.environ.get(\"HF_API_TOKEN\")\r\nif hf_token is None:\r\n    raise NameError(\"HF_API_TOKEN isn't defined\")\r\n\r\ndef get_llm_function(model_name=LLM_MODEL):\r\n    \"\"\"get LLM model between :\r\n    - mistralai/Mixtral-8x7B-Instruct-v0.1\r\n    - mistralai/Mistral-7B-Instruct-v0.1\r\n    - nvidia/Llama3-ChatQA-1.5-8B\r\n    - gpt-3.5-turbo\r\n    Other models can of course be implemented later\"\"\"\r\n\r\n    global global_model\r\n    if (model_name == \"mistralai/Mixtral-8x7B-Instruct-v0.1\" or model_name == \"mistralai/Mistral-7B-Instruct-v0.1\"):\r\n        if global_model is None:\r\n            cache_dir = \"C:/Users/Binec/PycharmProjects/RAG/.cache\"\r\n            tokenizer = AutoTokenizer.from_pretrained(model_name,\r\n                                                      trust_remote_code=True,\r\n                                                      token=hf_token,\r\n                                                      cache_dir=cache_dir)\r\n            tokenizer.pad_token = tokenizer.eos_token\r\n            tokenizer.padding_side = \"right\"\r\n\r\n            #################################################################\r\n            # bitsandbytes parameters\r\n            #################################################################\r\n            \"\"\"\r\n            # Activate 4-bit precision base model loading\r\n            use_4bit = True\r\n            \r\n            # Compute dtype for 4-bit base models\r\n            bnb_4bit_compute_dtype = \"float16\"\r\n            \r\n            # Quantization type (fp4 or nf4)\r\n            bnb_4bit_quant_type = \"nf4\"\r\n            \r\n            # Activate nested quantization for 4-bit base models (double quantization)\r\n            use_nested_quant = False\r\n            \r\n            #################################################################\r\n            # Set up quantization config\r\n            #################################################################\r\n            compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\r\n            \r\n            bnb_config = BitsAndBytesConfig(\r\n                load_in_4bit=use_4bit,\r\n                bnb_4bit_quant_type=bnb_4bit_quant_type,\r\n                bnb_4bit_compute_dtype=compute_dtype,\r\n                bnb_4bit_use_double_quant=use_nested_quant,\r\n            )\r\n            \r\n            # Check GPU compatibility with bfloat16\r\n            \r\n            if compute_dtype == torch.float16 and use_4bit:\r\n                major, _ = torch.cuda.get_device_capability()\r\n                if major >= 8:\r\n                    print(\"=\" * 80)\r\n                    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\r\n                    print(\"=\" * 80)\r\n            \"\"\"\r\n            model = AutoModelForCausalLM.from_pretrained(\r\n                model_name,\r\n                token=hf_token,\r\n                cache_dir=cache_dir,\r\n                # quantization_config = bnb_config\r\n            )\r\n\r\n            text_generation_pipeline = pipeline(\r\n                model=model,\r\n                tokenizer=tokenizer,\r\n                task=\"text-generation\",\r\n                temperature=0.2,\r\n                repetition_penalty=1.1,\r\n                return_full_text=True,\r\n                max_new_tokens=1000,\r\n            )\r\n\r\n            global_model = HuggingFacePipeline(pipeline=text_generation_pipeline)\r\n\r\n        return global_model\r\n\r\n    elif model_name == \"nvidia/Llama3-ChatQA-1.5-8B\":\r\n        if global_model is None:\r\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\r\n            text_generation_pipeline = pipeline(\r\n                model=model,\r\n                tokenizer=tokenizer,\r\n                task=\"text-generation\",\r\n                temperature=0.2,\r\n                repetition_penalty=1.1,\r\n                return_full_text=True,\r\n                max_new_tokens=1000,\r\n            )\r\n            global_model = HuggingFacePipeline(pipeline=text_generation_pipeline)\r\n        return global_model\r\n\r\n    elif model_name == \"gpt-3.5-turbo\":\r\n        from langchain_openai import ChatOpenAI\r\n        model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\r\n        return model\r\n\r\n    else:\r\n        # TODO create error message\r\n        print(\"Error no model founded\")\r\n        return\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python_script/get_llm_function.py b/python_script/get_llm_function.py
--- a/python_script/get_llm_function.py	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
+++ b/python_script/get_llm_function.py	(date 1731622252360)
@@ -27,7 +27,7 @@
     Other models can of course be implemented later"""
 
     global global_model
-    if (model_name == "mistralai/Mixtral-8x7B-Instruct-v0.1" or model_name == "mistralai/Mistral-7B-Instruct-v0.1"):
+    if model_name == "mistralai/Mixtral-8x7B-Instruct-v0.1" or model_name == "mistralai/Mistral-7B-Instruct-v0.1" or model_name == "crumb/nano-mistral":
         if global_model is None:
             cache_dir = "C:/Users/Binec/PycharmProjects/RAG/.cache"
             tokenizer = AutoTokenizer.from_pretrained(model_name,
@@ -36,11 +36,11 @@
                                                       cache_dir=cache_dir)
             tokenizer.pad_token = tokenizer.eos_token
             tokenizer.padding_side = "right"
-
+            """
             #################################################################
             # bitsandbytes parameters
             #################################################################
-            """
+
             # Activate 4-bit precision base model loading
             use_4bit = True
             
Index: local_app.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\r\nimport sys\r\n\r\nfrom flask import Flask, render_template, request, jsonify, send_from_directory\r\n\r\nlocal_app = Flask(__name__)\r\n\r\nsys.path.append(os.path.join(os.path.dirname(__file__), 'python_script'))\r\nfrom python_script.parameters import load_config\r\n\r\nglobal DATA_PATH\r\nload_config('test')\r\nfrom python_script.parameters import CHROMA_ROOT_PATH, EMBEDDING_MODEL, LLM_MODEL, PROMPT_TEMPLATE, DATA_PATH, \\\r\n    REPHRASING_PROMPT, STANDALONE_PROMPT, ROUTER_DECISION_PROMPT\r\nfrom python_script.get_llm_function import get_llm_function\r\nfrom python_script.get_rag_chain import get_rag_chain\r\nfrom python_script.ConversationalRagChain import ConversationalRagChain\r\n\r\n\r\ndef init_app():\r\n    load_rag()\r\n    local_app.config['UPLOAD_FOLDER'] = DATA_PATH\r\n\r\n\r\ndef load_rag(settings=None):\r\n    global rag_conv\r\n    print(settings)\r\n    if settings is None:\r\n        rag_conv = ConversationalRagChain.from_llm(\r\n            rag_chain=get_rag_chain(),\r\n            llm=get_llm_function(model_name=LLM_MODEL),\r\n            callbacks=None\r\n        )\r\n    else:\r\n        rag_conv = ConversationalRagChain.from_llm(\r\n            rag_chain=get_rag_chain(settings),\r\n            llm=get_llm_function(model_name=settings[\"llm_model\"]),\r\n            callbacks=None\r\n        )\r\n\r\n\r\n# Route to get the document list\r\n@local_app.route('/documents', methods=['GET'])\r\ndef list_documents():\r\n    files = os.listdir(local_app.config['UPLOAD_FOLDER'])\r\n    documents = [{\"name\": f, \"url\": f\"/files/{f}\", \"extension\": os.path.splitext(f)[1][1:]} for f in files]\r\n    return jsonify(documents)\r\n\r\n\r\n# Route to get a single document\r\n@local_app.route('/documents/<document_name>', methods=['GET'])\r\ndef get_document(document_name):\r\n    files = os.listdir(local_app.config['UPLOAD_FOLDER'])\r\n    documents = [{\"name\": f, \"url\": f\"/files/{f}\", \"extension\": os.path.splitext(f)[1][1:]} for f in files]\r\n\r\n    document = next((doc for doc in documents if doc[\"name\"] == document_name), None)\r\n\r\n    if document is None:\r\n        return jsonify({'error': 'Document not found'}), 404\r\n\r\n    return jsonify(document)\r\n\r\n\r\n# Route to show the pdf\r\n@local_app.route('/files/<filename>', methods=['GET'])\r\ndef serve_file(filename):\r\n    return send_from_directory(local_app.config['UPLOAD_FOLDER'], filename)\r\n\r\n\r\n@local_app.route(\"/\")\r\ndef index():\r\n    return render_template('local.html')\r\n\r\n\r\n@local_app.route(\"/get\", methods=[\"POST\"])\r\ndef chat():\r\n    data = request.get_json()\r\n    msg = data.get(\"msg\", \"\")\r\n    return get_Chat_response(msg)\r\n\r\n\r\ndef get_Chat_response(query):\r\n    print(\"query: \", str(query))\r\n    inputs = {\r\n        \"query\": str(query),\r\n        \"chat_history\": []\r\n    }\r\n    res = rag_conv._call(inputs)\r\n\r\n    output = jsonify({\r\n        'response': res['result'],\r\n        'context': res['context'],\r\n        'source': res['source']\r\n    })\r\n    print(res['result'])\r\n    return output\r\n\r\n\r\n@local_app.route('/update-settings', methods=['POST'])\r\ndef update_settings():\r\n    data = request.get_json()\r\n    load_rag(settings=data)\r\n    return jsonify({'status': 'success', 'message': 'Settings updated successfully'}), 200\r\n\r\n\r\n@local_app.route('/clear_chat_history', methods=['POST'])\r\ndef clear_chat_history():\r\n    rag_conv.clear_chat_history()\r\n    return jsonify({\"status\": \"success\"}), 200\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"1\")\r\n    init_app()\r\n    print(\"2\")\r\n    local_app.run(port=5000, debug=False)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/local_app.py b/local_app.py
--- a/local_app.py	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
+++ b/local_app.py	(date 1731621996761)
@@ -24,7 +24,6 @@
 
 def load_rag(settings=None):
     global rag_conv
-    print(settings)
     if settings is None:
         rag_conv = ConversationalRagChain.from_llm(
             rag_chain=get_rag_chain(),
@@ -112,5 +111,6 @@
 if __name__ == '__main__':
     print("1")
     init_app()
+
     print("2")
     local_app.run(port=5000, debug=False)
Index: .idea/shelf/Uncommitted_changes_before_rebase_[Changes]/shelved.patch
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_rebase_[Changes]/shelved.patch b/.idea/shelf/Uncommitted_changes_before_rebase_[Changes]/shelved.patch
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_rebase_[Changes]/shelved.patch	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
+++ /dev/null	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
@@ -1,18 +0,0 @@
-Index: .idea/RAG.iml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
-<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<module type=\"PYTHON_MODULE\" version=\"4\">\r\n  <component name=\"NewModuleRootManager\">\r\n    <content url=\"file://$MODULE_DIR$\">\r\n      <excludeFolder url=\"file://$MODULE_DIR$/.venv\" />\r\n    </content>\r\n    <orderEntry type=\"inheritedJdk\" />\r\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\r\n  </component>\r\n</module>
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/RAG.iml b/.idea/RAG.iml
---- a/.idea/RAG.iml	(revision ced62da6eb7d19a9da0cc7af5cdd7203af510daf)
-+++ b/.idea/RAG.iml	(date 1731602159681)
-@@ -3,6 +3,7 @@
-   <component name="NewModuleRootManager">
-     <content url="file://$MODULE_DIR$">
-       <excludeFolder url="file://$MODULE_DIR$/.venv" />
-+      <excludeFolder url="file://$MODULE_DIR$/data" />
-     </content>
-     <orderEntry type="inheritedJdk" />
-     <orderEntry type="sourceFolder" forTests="false" />
Index: .idea/shelf/Uncommitted_changes_before_rebase__Changes_.xml
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_rebase__Changes_.xml b/.idea/shelf/Uncommitted_changes_before_rebase__Changes_.xml
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_rebase__Changes_.xml	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
+++ /dev/null	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
@@ -1,4 +0,0 @@
-<changelist name="Uncommitted_changes_before_rebase_[Changes]" date="1731602214897" recycled="true" deleted="true">
-  <option name="PATH" value="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_rebase_[Changes]/shelved.patch" />
-  <option name="DESCRIPTION" value="Uncommitted changes before rebase [Changes]" />
-</changelist>
\ No newline at end of file
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"7c8b8920-ebb2-41a9-8f3b-1d300a7abb04\" name=\"Changes\" comment=\"First Version\">\r\n      <change afterPath=\"$PROJECT_DIR$/.gitignore\" afterDir=\"false\" />\r\n    </list>\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"Git.Rebase.Settings\">\r\n    <option name=\"NEW_BASE\" value=\"master\" />\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\r\n      <map>\r\n        <entry key=\"$PROJECT_DIR$\" value=\"master\" />\r\n      </map>\r\n    </option>\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"ProjectColorInfo\">{\r\n  &quot;associatedIndex&quot;: 6\r\n}</component>\r\n  <component name=\"ProjectId\" id=\"2onbXF2efEkRGBwB4IwRTbwDJSH\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\"><![CDATA[{\r\n  \"keyToString\": {\r\n    \"Python.local_app.executor\": \"Run\",\r\n    \"RunOnceActivity.OpenProjectViewOnStart\": \"true\",\r\n    \"RunOnceActivity.ShowReadmeOnStart\": \"true\",\r\n    \"git-widget-placeholder\": \"master\",\r\n    \"last_opened_file_path\": \"C:/Users/Binec/PycharmProjects/RAG\",\r\n    \"settings.editor.selected.configurable\": \"project.propVCSSupport.DirectoryMappings\"\r\n  }\r\n}]]></component>\r\n  <component name=\"RecentsManager\">\r\n    <key name=\"CopyFile.RECENT_KEYS\">\r\n      <recent name=\"C:\\Users\\Binec\\PycharmProjects\\RAG\\python_script\" />\r\n    </key>\r\n  </component>\r\n  <component name=\"SharedIndexes\">\r\n    <attachedChunks>\r\n      <set>\r\n        <option value=\"bundled-python-sdk-d68999036c7f-d3b881c8e49f-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-233.14475.56\" />\r\n      </set>\r\n    </attachedChunks>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"7c8b8920-ebb2-41a9-8f3b-1d300a7abb04\" name=\"Changes\" comment=\"\" />\r\n      <created>1731508324671</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1731508324671</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00001\" summary=\"First Version\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1731600985227</created>\r\n      <option name=\"number\" value=\"00001\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1731600985227</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00002\" summary=\"First Version\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1731602058170</created>\r\n      <option name=\"number\" value=\"00002\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1731602058170</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00003\" summary=\"First Version\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1731602289202</created>\r\n      <option name=\"number\" value=\"00003\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1731602289202</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00004\" summary=\"Second push\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1731602569859</created>\r\n      <option name=\"number\" value=\"00004\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1731602569859</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00005\" summary=\"Second push\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1731602661691</created>\r\n      <option name=\"number\" value=\"00005\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00005\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1731602661691</updated>\r\n    </task>\r\n    <option name=\"localTasksCounter\" value=\"6\" />\r\n    <servers />\r\n  </component>\r\n  <component name=\"Vcs.Log.Tabs.Properties\">\r\n    <option name=\"OPEN_GENERIC_TABS\">\r\n      <map>\r\n        <entry key=\"4f4d06f0-8f6f-4650-a681-f284acb79be2\" value=\"TOOL_WINDOW\" />\r\n      </map>\r\n    </option>\r\n    <option name=\"TAB_STATES\">\r\n      <map>\r\n        <entry key=\"4f4d06f0-8f6f-4650-a681-f284acb79be2\">\r\n          <value>\r\n            <State>\r\n              <option name=\"FILTERS\">\r\n                <map>\r\n                  <entry key=\"branch\">\r\n                    <value>\r\n                      <list>\r\n                        <option value=\"HEAD\" />\r\n                      </list>\r\n                    </value>\r\n                  </entry>\r\n                  <entry key=\"roots\">\r\n                    <value>\r\n                      <list>\r\n                        <option value=\"$PROJECT_DIR$\" />\r\n                      </list>\r\n                    </value>\r\n                  </entry>\r\n                </map>\r\n              </option>\r\n              <option name=\"SHOW_ONLY_AFFECTED_CHANGES\" value=\"true\" />\r\n            </State>\r\n          </value>\r\n        </entry>\r\n        <entry key=\"MAIN\">\r\n          <value>\r\n            <State />\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"VcsManagerConfiguration\">\r\n    <ignored-roots>\r\n      <path value=\"$PROJECT_DIR$\" />\r\n    </ignored-roots>\r\n    <MESSAGE value=\"Firsr Version\" />\r\n    <MESSAGE value=\"First Version\" />\r\n    <MESSAGE value=\"Second push\" />\r\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"Second push\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision de88562d4a42fb82d3d14d9f0faf2cf829ea42c2)
+++ b/.idea/workspace.xml	(date 1731622408636)
@@ -5,7 +5,11 @@
   </component>
   <component name="ChangeListManager">
     <list default="true" id="7c8b8920-ebb2-41a9-8f3b-1d300a7abb04" name="Changes" comment="First Version">
-      <change afterPath="$PROJECT_DIR$/.gitignore" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/local_app.py" beforeDir="false" afterPath="$PROJECT_DIR$/local_app.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/python_script/config.json" beforeDir="false" afterPath="$PROJECT_DIR$/python_script/config.json" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/python_script/get_llm_function.py" beforeDir="false" afterPath="$PROJECT_DIR$/python_script/get_llm_function.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/python_script/get_rag_chain.py" beforeDir="false" afterPath="$PROJECT_DIR$/python_script/get_rag_chain.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -23,6 +27,18 @@
     </option>
     <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
   </component>
+  <component name="GitHubPullRequestSearchHistory">{
+  &quot;lastFilter&quot;: {
+    &quot;state&quot;: &quot;OPEN&quot;,
+    &quot;assignee&quot;: &quot;Binec22&quot;
+  }
+}</component>
+  <component name="GithubPullRequestsUISettings">{
+  &quot;selectedUrlAndAccountId&quot;: {
+    &quot;url&quot;: &quot;https://github.com/Binec22/RAG.git&quot;,
+    &quot;accountId&quot;: &quot;306938fa-22f4-44c0-a39c-f69d41e0c65d&quot;
+  }
+}</component>
   <component name="ProjectColorInfo">{
   &quot;associatedIndex&quot;: 6
 }</component>
@@ -32,16 +48,24 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent"><![CDATA[{
-  "keyToString": {
-    "Python.local_app.executor": "Run",
-    "RunOnceActivity.OpenProjectViewOnStart": "true",
-    "RunOnceActivity.ShowReadmeOnStart": "true",
-    "git-widget-placeholder": "master",
-    "last_opened_file_path": "C:/Users/Binec/PycharmProjects/RAG",
-    "settings.editor.selected.configurable": "project.propVCSSupport.DirectoryMappings"
+  <component name="PropertiesComponent">{
+  &quot;keyToString&quot;: {
+    &quot;Python.local_app.executor&quot;: &quot;Run&quot;,
+    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
+    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
+    &quot;RunOnceActivity.git.unshallow&quot;: &quot;true&quot;,
+    &quot;git-widget-placeholder&quot;: &quot;master&quot;,
+    &quot;ignore.virus.scanning.warn.message&quot;: &quot;true&quot;,
+    &quot;last_opened_file_path&quot;: &quot;C:/Users/Binec/PycharmProjects/RAG&quot;,
+    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,
+    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,
+    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,
+    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,
+    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,
+    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;,
+    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;
   }
-}]]></component>
+}</component>
   <component name="RecentsManager">
     <key name="CopyFile.RECENT_KEYS">
       <recent name="C:\Users\Binec\PycharmProjects\RAG\python_script" />
@@ -50,7 +74,8 @@
   <component name="SharedIndexes">
     <attachedChunks>
       <set>
-        <option value="bundled-python-sdk-d68999036c7f-d3b881c8e49f-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-233.14475.56" />
+        <option value="bundled-js-predefined-d6986cc7102b-e768b9ed790e-JavaScript-PY-243.21565.199" />
+        <option value="bundled-python-sdk-cab1f2013843-4ae2d6a61b08-com.jetbrains.pycharm.pro.sharedIndexes.bundled-PY-243.21565.199" />
       </set>
     </attachedChunks>
   </component>
@@ -62,6 +87,8 @@
       <option name="number" value="Default" />
       <option name="presentableId" value="Default" />
       <updated>1731508324671</updated>
+      <workItem from="1731605549395" duration="1524000" />
+      <workItem from="1731617095954" duration="4653000" />
     </task>
     <task id="LOCAL-00001" summary="First Version">
       <option name="closed" value="true" />
@@ -106,6 +133,9 @@
     <option name="localTasksCounter" value="6" />
     <servers />
   </component>
+  <component name="TypeScriptGeneratedFilesManager">
+    <option name="version" value="3" />
+  </component>
   <component name="Vcs.Log.Tabs.Properties">
     <option name="OPEN_GENERIC_TABS">
       <map>
@@ -156,4 +186,7 @@
     <MESSAGE value="Second push" />
     <option name="LAST_COMMIT_MESSAGE" value="Second push" />
   </component>
+  <component name="com.intellij.coverage.CoverageDataManagerImpl">
+    <SUITE FILE_PATH="coverage/RAG$local_app.coverage" NAME="local_app Coverage Results" MODIFIED="1731617157502" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+  </component>
 </project>
\ No newline at end of file
